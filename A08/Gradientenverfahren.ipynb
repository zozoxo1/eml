{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c211d38a"
      },
      "source": [
        "<figure>\n",
        "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
        "</figure>\n",
        "\n",
        "# Einführung Machine Learning\n",
        "### Sommersemester 2022\n",
        "Prof. Dr. Heiner Giefers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZppIM7L0rwUc"
      },
      "source": [
        "# Das Gradientenverfahren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dGQrBDS6rwUd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jp77CxcSrwUe"
      },
      "outputs": [],
      "source": [
        "# Test Case\n",
        "m_c, n_c = 2, 4\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(m_c, n_c)\n",
        "theta_c = np.r_[[[0]],np.random.randn(n_c, 1)]\n",
        "y_c = np.random.randn(m_c, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Ni7JLjrwUe"
      },
      "source": [
        "Bei der Logistischen Regression haben wir das Gradientenverfahren benutzt, um die Parameter unseres Modells, einer linearen Funktion $$Z = f_{\\theta}(x)=\\theta_0+\\theta_ix$$ transformiert durch die Aktivierungsfunktion $$\\hat y = h_{\\theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$ schrittweise zu verbessern.\n",
        "\n",
        "\n",
        "Die Verbesserung, bzw. die Qualität des Modells, haben wir anhand der Kostenfunktion $$J_{\\theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$ berechnet.\n",
        "\n",
        "Wir wollen nun schrittweise die Modellfunktion, die Kostenfunktion sowie das Gradientenverfahren als Python-Funktionen definieren.\n",
        "Um uns die Berechnungen zu vereinfachen, hängen wir an unseren Datensatz eine Spalte mit Einsen an.\n",
        "Damit können wir alle Parameter $\\theta$ (inklusive des Bias-Parameters) in einer Vektor-Operation verarbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEMg05l4rwUf",
        "outputId": "b4fcdadb-a434-4345-cb88-d296f4171fcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  1.76405235,  0.40015721,  0.97873798,  2.2408932 ],\n",
              "       [ 1.        ,  1.86755799, -0.97727788,  0.95008842, -0.15135721]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X_c = np.c_[np.ones(X.shape[0]).T,X]\n",
        "X_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0tAAaD2rwUf"
      },
      "source": [
        "## Modellfunktion\n",
        "**Aufgabe: Schreibe eine Funktion $f$, die folgende Parameter erhält:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält.\n",
        "2. Die Parameter $\\theta$\n",
        "\n",
        "**$f$ soll folgende lineare Funktion implementieren:** $$Z = f_{\\theta}(X)=X\\theta$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a46e169efb324584",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "hhkbvD04rwUg"
      },
      "outputs": [],
      "source": [
        "def f(X, theta):\n",
        "    \"\"\"evaluates linear function.\n",
        "    Arguments:\n",
        "        X: value\n",
        "        theta: Parameter\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.matmul(X, theta)\n",
        "    \n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-798b7b367b206c2f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "TjjBQT9QrwUg"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "Z = f(X_c, theta_c)\n",
        "#----------\n",
        "# f\n",
        "\n",
        "assert Z.shape == (m_c, 1), 'Use correctly sequenced matrix multiplication'\n",
        "assert np.isclose(Z[0], 3.38207), 'Expected 3.38207 but got %.5f' %Z[0]\n",
        "\n",
        "del Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF93oCEErwUh"
      },
      "source": [
        "**Aufgabe: Implementieren Sie die Modellunktion $h$. Die Funktion $h$ soll die gleichen Parameter wie $f$ erhalten und die Funktion $f$ intern aufrufen. Das Ergebnis von $f$ soll durch die Sigmoid-Aktivierungsfunktion transformiert werden.:** $$\\hat y = h_{\\theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-aed34bc92bba34fb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "FdufYn3grwUh"
      },
      "outputs": [],
      "source": [
        "def h(X, theta):\n",
        "    \"\"\"returns the sigmoid of the linear function.\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta: Parameters\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = f(X, theta)\n",
        "\n",
        "    y_hat = 1/(1+np.exp(-Z))\n",
        "    \n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3680cf0b32123e22",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Y3VBNsObrwUh"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "y_hat = h(X_c, theta_c)\n",
        "#----------\n",
        "# h\n",
        "\n",
        "assert y_hat.shape == (m_c, 1)\n",
        "assert np.isclose(y_hat[0], 0.96713), 'Expected 0.96713 but got %.5f' %y_hat[0]\n",
        "\n",
        "del y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JqQUByHrwUh"
      },
      "source": [
        "**Aufgabe: Berechnen Sie nun die Kostenfunktion. Schreiben Sie eine Funktion $J$, die folgende Parameter erhält:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\theta$.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "\n",
        "**$J$ berechnet die folgende Kostenfunktion:** $$J_{\\theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2441e8b76f6c8b3b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "teSzNHLirwUi"
      },
      "outputs": [],
      "source": [
        "def J(X,theta,y):\n",
        "    \"\"\"computes the Cross-entropy cost function\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta: Parameter\n",
        "        y: True labels\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(X)\n",
        "    y_hat = h(X, theta)\n",
        "    result_sum = 0\n",
        "    for i in range(1, m + 1):\n",
        "      result_sum += ((y[i-1]) * (np.log((y_hat[i-1])))\n",
        "      + ((1 - (y[i-1])) * np.log(1 - (y_hat[i-1]))))\n",
        "\n",
        "    result = (-(1 / m)) * result_sum\n",
        "\n",
        "    return result.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-6d386b0ed985b5a1",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Ob17jAeTrwUi"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "cost = J(X_c,theta_c,y_c)\n",
        "#----------\n",
        "# J\n",
        "\n",
        "assert cost.shape == (), 'Use correctly sequenced matrix multiplication'\n",
        "assert np.isclose(cost, 0.66739), 'Expected 0.66739but got %.5f' %cost\n",
        "\n",
        "del cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I8JcNw5rwUi"
      },
      "source": [
        "## Gradientenverfahren\n",
        "\n",
        "**Schreibe für das Gradientenverfahren eine Funktion `grads`, die folgende Parameter erhält**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\theta$.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "\n",
        "**und den Gradienten $\\partial\\theta$ für die Parameter $\\theta$ berechnet** \n",
        "\n",
        "Dabei ist $\\partial\\theta$ ein Vektor der Dimension `(n+1, 1)` mit den Gradienten der Parameter: $$ \\partial \\theta = \\frac{1}{m}X^T(\\hat y-y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-467565ebb154795c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NAxVyKEjrwUi"
      },
      "outputs": [],
      "source": [
        "def grads(X,theta,y):\n",
        "    \"\"\"Berechnet die Gradienten der Kostenfunktion abhängig von dern Parametern.\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta: Parameter\n",
        "        y: True labels\n",
        "    \"\"\"\n",
        "    \n",
        "    m = len(X)\n",
        "    y_hat = h(X, theta)\n",
        "    xt = np.matrix.transpose(X)\n",
        "\n",
        "    print(xt)\n",
        "    print(np.subtract(y_hat, y))\n",
        "\n",
        "    dtheta = (1 / m) * np.matmul(xt, np.subtract(y_hat, y))\n",
        "    \n",
        "    return dtheta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d5aaa7214b2dfbcd",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87xNgeFGrwUj",
        "outputId": "5be7613c-4901-4093-f813-fb079308cdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.          1.        ]\n",
            " [ 1.76405235  1.86755799]\n",
            " [ 0.40015721 -0.97727788]\n",
            " [ 0.97873798  0.95008842]\n",
            " [ 2.2408932  -0.15135721]]\n",
            "[[0.20610183]\n",
            " [0.21518991]]\n"
          ]
        }
      ],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "dt = grads(X_c,theta_c,y_c)\n",
        "#----------\n",
        "# grads\n",
        "\n",
        "assert dt.shape == theta_c.shape\n",
        "assert np.isclose(dt[1], 0.38273), 'Expected 0.38273 but got %.5f' %dt[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao7n7Me-rwUj"
      },
      "source": [
        "**Aufgabe: Schreiben Sie nun eine Funktion, die die Modellparameter aufgrund der berechneten Gradienten aktualisiert.Die Funktion `update`erhält die Parameter $\\theta$, die Gradienten $\\partial \\theta$ sowie die Lernrate $\\alpha$ und berechnet:**\n",
        "\n",
        "$$\\theta = \\theta - \\alpha \\cdot \\partial \\theta$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9c9c665b5ef00354",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-OP7rBlbrwUj"
      },
      "outputs": [],
      "source": [
        "def update(theta, dtheta, alpha):\n",
        "    \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    \n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-628e4db830095cf8",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "NXuY-kvmrwUj"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "t = update(theta_c, dt, 0.1)\n",
        "#----------\n",
        "# update\n",
        "\n",
        "assert t.shape == theta_c.shape\n",
        "assert np.isclose(t[1], -0.141491), 'Expected -0.141491 but got %.5f' %t[1]\n",
        "del t, dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HM-n04OrwUj"
      },
      "source": [
        "Nun können wir das iterative Gradientenverfahren programmieren.\n",
        "**Aufgabe: Schreiben Sie eine Funktion `gradient_descent`, die folgende Parameter erhät:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\theta$.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "4. Die Lernrate $\\alpha$.\n",
        "5. Die Anzahl der Iterationen.\n",
        "**Die Funktion soll die Trainierten Modellparameter $\\theta$ zurückgeben.**\n",
        "\n",
        "*Hinweis*: Berechnen Sie Kosten mit der Funktion `J` und hängen Sie diese Kosten nach jedem Berechnungsschritt and die Liste `cost` an $\\rightarrow$ Berechnen Sie die Gradienten mit der Funktion `grads` $\\rightarrow$ Verwenden Sie diese Gradienten um die Parameter mit der Funktion `update`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c950a15eb73cda45",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "t8jL1S_srwUj"
      },
      "outputs": [],
      "source": [
        "def gradient_decent(X, theta, y, alpha=0.1, iterations=100):\n",
        "    \"\"\"performs gradient decent optimization.\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta: Parameter\n",
        "        y: True labels\n",
        "        alpha(default=0.1): Learning rate\n",
        "        iterations(default=100): number of updating iterations\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    return theta, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-9e057a811aa97078",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "m9k9nut1rwUk"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "t, costs = gradient_decent(X_c, theta_c, y_c)\n",
        "#----------\n",
        "# gradient_decent\n",
        "\n",
        "assert len(costs) == 100, 'Make sure to calculate and append the cost in every iteration.'\n",
        "assert np.isclose(t[4], 1.05769), 'Expected 1.05186 but got %.5f' %t[4]\n",
        "plt.plot([i for i in range(len(costs))],costs)\n",
        "del t, costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o7Qfs0zrwUk"
      },
      "source": [
        "## Anwendung der Funktionen auf einen realistischen Datensatz\n",
        "\n",
        "Wir habe nun alle Funktionen um unser logistisches Regressionsmodell für einen *echten* Datensatz zu einzusetzen.\n",
        "Wir verwenden hier den Brustkrebs-Datensatz aus Sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ThiatuWrwUk"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,test_size=0.3)\n",
        "\n",
        "# preprocessing\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "y_train = np.expand_dims(y_train, 1)\n",
        "y_test = np.expand_dims(y_test, 1)\n",
        "\n",
        "X_train = np.c_[np.ones(X_train.shape[0]).T,X_train]\n",
        "X_test = np.c_[np.ones(X_test.shape[0]).T,X_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix9qaHzPrwUk"
      },
      "outputs": [],
      "source": [
        "# initializing parameters\n",
        "theta = np.zeros((len(X_train[0]), 1))\n",
        "\n",
        "#training the model\n",
        "theta, costs = gradient_decent(X_train, theta, y_train.reshape(-1, 1), alpha = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yDSKU86rwUk"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xc9hZoWrwUk"
      },
      "outputs": [],
      "source": [
        "# measuring performance\n",
        "y_pred = (h(X_test,theta) >= 0.5)*1\n",
        "acc = 100-np.sum(np.abs(y_pred-y_test))*100/len(y_test)\n",
        "\n",
        "print(\"Die classifcation accuracy ist: \",acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Gradientenverfahren.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}